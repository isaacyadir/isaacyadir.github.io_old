<!DOCTYPE html>
<html lang="en-us">
  <head>
    <script defer src="https://use.fontawesome.com/releases/v5.13.0/js/all.js"></script>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.58.3" />


<title>PyData Berlin 2018: On Laplacian Eigenmaps for Dimensionality Reduction - Dr. Juan Camilo Orduz</title>
<meta property="og:title" content="PyData Berlin 2018: On Laplacian Eigenmaps for Dimensionality Reduction - Dr. Juan Camilo Orduz">


  <link href='../images/favicon.ico' rel='icon' type='image/x-icon'/>



  








<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="../css/fonts.css" media="all">
<link rel="stylesheet" href="../css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="../" class="nav-logo">
    <img src="../images/sphere2.gif"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="../about/"> About</a></li>
    
    <li><a href="https://github.com/juanitorduz"><i class='fab fa-github fa-lg'></i>  </a></li>
    
    <li><a href="https://www.linkedin.com/in/juanitorduz/"><i class='fab fa-linkedin fa-lg'></i>  </a></li>
    
    <li><a href="https://twitter.com/juanitorduz"><i class='fab fa-twitter fa-lg'></i>  </a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">6 min read</span>
    

    <h1 class="article-title">PyData Berlin 2018: On Laplacian Eigenmaps for Dimensionality Reduction</h1>

    
    <span class="article-date">2018-07-08</span>
    

    <div class="article-content">
      

<p>This summer I had the great oportunity to attend an give a talk at <a href="https://pydata.org/berlin2018/">Pydata Berlin 2018</a>. The topic of my talk was <a href="https://pydata.org/berlin2018/schedule/presentation/33/">On Laplacian Eigenmaps for Dimensionality Reduction</a>. During the <em>Unsupervised Learning &amp; Visualization</em> session Dr. Stefan Kühn presented a very interesting and visual talk on <a href="https://pydata.org/berlin2018/schedule/presentation/55/">Manifold Learning and Dimensionality Reduction for Data Visualization and Feature Engineering</a> and Dr. Evelyn Trautmann gave a nice application of spectral clustering (<a href="http://www.kyb.mpg.de/fileadmin/user_upload/files/publications/attachments/Luxburg07_tutorial_4488%5b0%5d.pdf">here</a> are some nice notes around this subject) in the context of <a href="https://pydata.org/berlin2018/schedule/presentation/48/">Extracting relevant Metrics</a>. During our talks, spectral methods and the <a href="https://en.wikipedia.org/wiki/Laplacian_matrix">Graph Laplacian</a> was the biggest intersection. This was an excelent oportunity to enhance the communication channels between pure math and its applications.</p>

<p>I want to acknowledge the organizers, volunteers and great speakers which made this conference possible.</p>

<hr />

<p><strong>Abstract</strong></p>

<p>The aim of this talk is to describe a non-linear dimensionality reduction algorithm based on spectral techniques introduced in <a href="http://web.cse.ohio-state.edu/~belkin.8/papers/LEM_NC_03.pdf">BN2003</a>. The goal of non-linear dimensionality reduction algorithms, so called  <em>manifold learning algorithms</em>, is to construct a representation of data on a low dimensional manifold embedded in a high dimensional space. The particular case we are going to present exploits various relations of geometric and spectral methods (discrete and continuous). Spectral methods are sometimes motivated by Marc Kac&rsquo;s question  <em>Can One Hear the Shape of a Drum?</em> which makes reference to the idea of recovering geometrical properties from the eigenvalues (spectrum) of a matrix (linear operator). Concretely, the approach followed in <a href="http://web.cse.ohio-state.edu/~belkin.8/papers/LEM_NC_03.pdf">BN2003</a> has its foundation on the spectral analysis of the <a href="https://en.wikipedia.org/wiki/Laplacian_matrix">Graph Laplacian</a> of the adjacency graph constructed from the data. The motivation of the construction relies on the continuous limit analogue, the <a href="https://en.wikipedia.org/wiki/Laplace–Beltrami_operator">Laplace-Beltrami</a> operator, in providing an optimal embedding for manifolds. We will also indicate the relation with the associated heat kernel operator. Instead of following a pure formal approach we will present the main geometric and computational ideas of the algorithm. Hence, with basic knowledge of linear algebra (eigenvalues) and differential calculus you will be able to follow the talk. Finally we will show a concrete example in Python using <a href="http://scikit-learn.org/stable/modules/generated/sklearn.manifold.SpectralEmbedding.html">scikit-learn</a>.</p>

<hr />

<p><strong>Video</strong>
<center>
  <iframe width="500" height="300" src="https://www.youtube.com/embed/U31TIICsHiA?rel=0" frameborder="0" allowfullscreen> </iframe></p>

<h2 id="center"></center></h2>

<p><strong>Slides</strong></p>

<p><a href="../documents/orduz_pydata2018.pdf">Here</a> you can find the slides of the talk.</p>

<p>Suggestions and comments are always welcome!</p>

<hr />

<h2 id="jupyter-notebook">Jupyter Notebook</h2>

<p>In this notebook we present some simple examples which ilustrate the concepts discussed in the talk on <a href="https://pydata.org/berlin2018/schedule/presentation/33/">Laplacian Eigenmaps for Dimensionality Reduction</a> at <a href="https://pydata.org/berlin2018/">PyData Berlin 2018</a>.</p>

<p>The sample data, visualization code and object descriptions can be found in the documentation.</p>

<p><strong>References:</strong></p>

<ul>
<li><p><a href="http://scikit-learn.org/stable/modules/generated/sklearn.manifold.spectral_embedding.html">http://scikit-learn.org/stable/modules/generated/sklearn.manifold.spectral_embedding.html</a></p></li>

<li><p><a href="http://scikit-learn.org/stable/modules/manifold.html#spectral-embedding">http://scikit-learn.org/stable/modules/manifold.html#spectral-embedding</a></p></li>
</ul>

<h2 id="1-notebook-setup">1. Notebook Setup</h2>

<pre><code class="language-python">import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import numpy as np
from sklearn import manifold, datasets
from sklearn.utils import check_random_state
from sklearn.decomposition import PCA
</code></pre>

<h2 id="2-back-to-our-toy-example">2. Back to Our Toy Example</h2>

<p><center>
<img src="../images/toy_graph.png" alt="html" style="width: 400px;"/>
</center></p>

<p>Let us compute the graph Laplacian from the adjacency matrix.</p>

<pre><code class="language-python"># Construct the adjacency matrix from the graph for n_neighbors = 1. 
W = np.array([[0, 1, 1, 1],
              [1, 0, 0, 0], 
              [1, 0, 0, 0], 
              [1, 0, 0, 0]])

# Construct the degree matrix D from W.
D = np.diag(np.apply_along_axis(arr=W,
                                func1d=np.sum,
                                axis=0))
# Compute the graph Laplacian.
L = D - W

print(L)
</code></pre>

<pre><code>[[ 3 -1 -1 -1]
 [-1  1  0  0]
 [-1  0  1  0]
 [-1  0  0  1]]
</code></pre>

<p>Let us use <code>manifold.spectral_embedding</code> to compute the projection onto \(\mathbb{R}\).</p>

<pre><code class="language-python">y = manifold.spectral_embedding(adjacency=W, 
                                n_components=1, 
                                norm_laplacian=False, 
                                drop_first=True,
                                eigen_solver='lobpcg')

print(y)
</code></pre>

<pre><code>[[ 0.        ]
 [-0.57735027]
 [ 0.78867513]
 [-0.21132487]]
</code></pre>

<p>Let us verify the result. Recall that the first non zero eigenvalue of is \(\lambda_1 = 1\).</p>

<pre><code class="language-python">lambda_1 = 1

# Check if the previous output coincides with the analytical requirement. 
np.array_equal(a1=np.dot(L, y), 
               a2=lambda_1*np.dot(D, y))
</code></pre>

<pre><code>True
</code></pre>

<p>Thus, \(y\) is a solution of \(Lf = \lambda_1 Df\) with \(\lambda_1 =1\).</p>

<h2 id="3-higher-dimensional-examples">3 Higher Dimensional Examples</h2>

<h3 id="3-1-s-curve">3.1 S Curve</h3>

<p><strong>Reference:</strong> <a href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_s_curve.html">http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_s_curve.html</a></p>

<pre><code class="language-python"># Define the number of points to consider. 
n_points = 3000

# Get the data and color map. 
S_curve, S_colors = datasets.samples_generator.make_s_curve(n_points, random_state=0)

Axes3D

fig = plt.figure(figsize=(12, 12))
ax = fig.add_subplot(111, projection='3d')
ax.scatter(S_curve[:, 0], S_curve[:, 1], S_curve[:, 2],
           c=S_colors, 
           cmap=plt.cm.Spectral)
ax.view_init(4, -72);
</code></pre>

<p><center>
<img src="../images/spectral_embedding_v2_files/spectral_embedding_v2_12_0.png" alt="png" />
</center></p>

<h4 id="3-1-1-pca">3.1.1 PCA</h4>

<p>Let us see the result of PCA with <code>n_components=2</code>.</p>

<pre><code class="language-python"># Fit PCA object.
S_curve_pca = PCA(n_components=2).fit_transform(S_curve)

# Visualize the result.
fig = plt.figure(figsize=(12, 6))
ax = fig.add_subplot(111)
ax.scatter(S_curve_pca[:, 0], S_curve_pca[:, 1],
           c=S_colors, 
           cmap=plt.cm.Spectral);
</code></pre>

<p><center>
<img src="../images/spectral_embedding_v2_files/spectral_embedding_v2_14_0.png" alt="png" />
</center></p>

<p>The result is coincides with ouur intuition, it is basically projecting to an axis so that the variance is maximized.</p>

<h4 id="3-1-2-spectral-embedding">3.1.2 Spectral Embedding</h4>

<p>Now let us use the method <code>SpectralEmbedding</code> from <code>sklearn</code>.</p>

<pre><code class="language-python"># Set the number of n_neighbors;
n_neighbors = 6 

# Set the dimension of the target space. 
n_components = 2

# Construct the SpectralEmbedding object. 
se = manifold.SpectralEmbedding(n_components=n_components,
                                affinity= 'nearest_neighbors',
                                n_neighbors=n_neighbors)

# Fit the SpectralEmbedding object. 
S_curve_red = se.fit_transform(X=S_curve)

# Visualize the result.
fig = plt.figure(figsize=(12, 6))
ax = fig.add_subplot(111)
ax.scatter(S_curve_red[:, 0], S_curve_red[:, 1],
           c=S_colors, 
           cmap=plt.cm.Spectral);
</code></pre>

<p><center>
<img src="../images/spectral_embedding_v2_files/spectral_embedding_v2_17_0.png" alt="png" />
</center></p>

<p>We see that <code>SpectralEmbedding</code> <em>unroll</em> the S-curve so that the locallity is preserved on average.</p>

<p>Now let us consider <code>n_neighbors</code> &gt;&gt; 1.</p>

<pre><code class="language-python"># Set the number of n_neighbors;
n_neighbors = 1500 

# Set the dimension of the target space. 
n_components = 2

# Construct the SpectralEmbedding object. 
se = manifold.SpectralEmbedding(n_components=n_components,
                                affinity= 'nearest_neighbors',
                                n_neighbors=n_neighbors)

# Fit the SpectralEmbedding object. 
S_curve_red = se.fit_transform(X=S_curve)

# Visualize the result.
fig = plt.figure(figsize=(12, 6))
ax = fig.add_subplot(111)
ax.scatter(S_curve_red[:, 0], S_curve_red[:, 1],
           c=S_colors, 
           cmap=plt.cm.Spectral);
</code></pre>

<p><center>
<img src="../images/spectral_embedding_v2_files/spectral_embedding_v2_20_0.png" alt="png" />
</center></p>

<p>The result is that the problem becomes more <em>global</em>. Thus, we get a similar result as PCA.</p>

<h3 id="3-2-two-dimensional-sphere-s-2">3.2 Two Dimensional Sphere \(S^2\)</h3>

<p><strong>Reference:</strong> <a href="http://scikit-learn.org/stable/auto_examples/manifold/plot_manifold_sphere.html">http://scikit-learn.org/stable/auto_examples/manifold/plot_manifold_sphere.html</a></p>

<p>First we generate the data.</p>

<pre><code class="language-python"># Create our sphere.
n_samples = 5000

angle_parameter = 0.5 # Try 0.01
pole_hole_parameter = 8 # Try 50

random_state = check_random_state(0)
p = random_state.rand(n_samples) * (2 * np.pi - angle_parameter)
t = random_state.rand(n_samples) * np.pi

# Sever the poles from the sphere.
indices = ((t &lt; (np.pi - (np.pi / pole_hole_parameter))) &amp; 
           (t &gt; ((np.pi / pole_hole_parameter))))
colors = p[indices]
x, y, z = np.sin(t[indices]) * np.cos(p[indices]), \
          np.sin(t[indices]) * np.sin(p[indices]), \
          np.cos(t[indices])
        
sphere_data = np.array([x, y, z]).T
</code></pre>

<p>Let us visualize the data.</p>

<pre><code class="language-python">fig = plt.figure(figsize=(12, 12))
ax = fig.add_subplot(111, projection='3d')
ax.scatter(x, y, z,
           c=colors, 
           cmap=plt.cm.Spectral);
</code></pre>

<p><center>
<img src="../images/spectral_embedding_v2_files/spectral_embedding_v2_25_0.png" alt="png" />
</center></p>

<h4 id="3-2-1-pca">3.2.1 PCA</h4>

<p>Again, let us begin using PCA.</p>

<pre><code class="language-python">sphere_pca = PCA(n_components=2).fit_transform(sphere_data)

fig = plt.figure(figsize=(10, 10))
ax = fig.add_subplot(111)
ax.scatter(sphere_pca[:, 0], sphere_pca[:, 1],
           c=colors, 
           cmap=plt.cm.Spectral);
</code></pre>

<p><center>
<img src="../images/spectral_embedding_v2_files/spectral_embedding_v2_27_0.png" alt="png" />
</center></p>

<h4 id="3-3-2-spectral-embedding">3.3.2 Spectral Embedding</h4>

<pre><code class="language-python">n_neighbors = 8
n_components = 2

# Fit the object.
se = manifold.SpectralEmbedding(n_components=n_components,
                                affinity='nearest_neighbors',
                                n_neighbors=n_neighbors)

sphere_data_red = se.fit_transform(X=sphere_data)
</code></pre>

<p>Let us visualize the results.</p>

<pre><code class="language-python">fig = plt.figure(figsize=(10, 10))
ax = fig.add_subplot(111)
ax.scatter(sphere_data_red[:, 0], sphere_data_red[:, 1],
           c=colors, 
           cmap=plt.cm.Spectral);
</code></pre>

<p><center>
<img src="../images/spectral_embedding_v2_files/spectral_embedding_v2_31_0.png" alt="png" />
</center></p>

<p>We see how <code>SpectralEmbedding</code> unrolls the sphere.</p>

<p>Finally, let us take again <code>n_neighbors</code> &gt;&gt; 1.</p>

<pre><code class="language-python">n_neighbors = 1000

# Fit the object.
se = manifold.SpectralEmbedding(n_components=n_components,
                                affinity='nearest_neighbors',
                                n_neighbors=n_neighbors)

sphere_data_red = se.fit_transform(X=sphere_data)

fig = plt.figure(figsize=(10, 10))
ax = fig.add_subplot(111)
ax.scatter(sphere_data_red[:, 0], sphere_data_red[:, 1],
           c=colors, 
           cmap=plt.cm.Spectral);
</code></pre>

<p><center>
<img src="../images/spectral_embedding_v2_files/spectral_embedding_v2_34_0.png" alt="png" />
</center></p>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="../index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="../images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/python.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/dockerfile.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/bash.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="../js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-122570825-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

  </body>
</html>


<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>python, scikit-learn, machine_learning, interpretable_ml on Dr. Juan Camilo Orduz</title>
    <link>/tags/python-scikit-learn-machine_learning-interpretable_ml/</link>
    <description>Recent content in python, scikit-learn, machine_learning, interpretable_ml on Dr. Juan Camilo Orduz</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 01 Jul 2021 00:00:00 +0000</lastBuildDate><atom:link href="/tags/python-scikit-learn-machine_learning-interpretable_ml/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Exploring Tools for Interpretable Machine Learning</title>
      <link>/interpretable_ml/</link>
      <pubDate>Thu, 01 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>/interpretable_ml/</guid>
      <description>TIn this notebook we want to test various ways of getting a better understanding on how non-trivial machine learning models generate predictions and how features interact with each other. This is in general not straight forward and key components are (1) understanding on the input data and (2) domain knowledge on the problem. Two great references on the subject are:
 Interpretable Machine Learning, A Guide for Making Black Box Models Explainable by Christoph Molnar Interpretable Machine Learning with Python by Serg Mas√≠s  Note that the methods discussed in this notebook are not related with causality.</description>
    </item>
    
  </channel>
</rss>
